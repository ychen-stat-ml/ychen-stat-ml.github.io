---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hey, I am Yifan Chen, an Assistant Professor in Computer Science and Mathematics (affiliate) at Hong Kong Baptist University. 

I obtained my Ph.D. degree from Department of Statistics, University of Illinois Urbana-Champaign in 2023, advised by Prof. <a href="https://sites.google.com/site/yunyangstat/">Yun Yang</a>. 
I fortunately work with Prof. <a href="https://sites.google.com/site/yunyangstat/">Yun Yang</a>, 
Prof. <a href="https://stat.illinois.edu/directory/profile/rqzhu/">Ruoqing Zhu</a>, 
Prof. <a href="https://cs.illinois.edu/about/people/faculty/hengji/">Heng Ji</a>,
and Prof. <a href="https://ischool.illinois.edu/people/jingrui-he/">Jingrui He</a> during my Ph.D. studies.
I also collaborated with Prof. <a href="https://www.linkedin.com/in/dilek-hakkani-tur-9517543/">Dilek Hakkani-Tur</a> at Amazon Alexa AI, and Dr. <a href="https://research.ibm.com/people/jie-chen">Jie Chen</a> at IBM Research.
Before that, I received my B.S. degree in Statistics from Fudan University in 2018, advised by Prof. <a href="https://www.fdsm.fudan.edu.cn/en/teacher/preview.aspx?UID=141685">Juan Shen</a> and Prof. <a href="https://www.fdsm.fudan.edu.cn/en/teacher/preview.aspx?UID=1859">Chenghong Zhang</a>.

# ğŸ’» Research

I am broadly interested in the general area of **efficient machine learning**, aiming to understand the statistical structures of modern machine learning algorithms and apply these insights to real-world computational challenges.
I especially focus on <strong>non-parametric models</strong> and neural networks with intensive matrix operations, such as <strong>Transformers</strong> (language models) and graph neural networks (<strong>GNNs</strong>).


# ğŸ•® Teaching

## Spring 25, Spring 24: COMP 7070 Advanced Topics in Artificial Intelligence and Machine Learning

Overall, this course is an invitation to core machine learning for **AI application research**. It aims to familiarize the students with useful concepts in machine learning, and therefore to benefit their own research.

> Studies related to machine learning is implicitly divided into three genres in this statement: learning theory, core machine learning, and AI application.

Scribed lecture notes can be found on [this page](/teaching.html).

## Fall 24: COMP 2027 Applied Linear Algebra for Computing


# ğŸ”¥ News

- *2025.5*: &nbsp;ğŸ‰ğŸ‰ Two papers were accepted to ICML 2025! Congratulations to Prof. Weizhong Zhang's group at Fudan and Tianyi!
- *2025.4*: &nbsp;ğŸ‰ğŸ‰ One paper was accepted to ISIT 2025 (the flagship conference of the IEEE Information Theory Society)! Congratulations to Peng!
- *2024.12*: &nbsp;ğŸ‰ğŸ‰ One paper was accepted to AAAI 2025, and another to ICASSP 2025 (a comprehensive conference focused on signal processing and its applications)! Congratulations to Prof. Xiaoqiang Li's group at SHU and Tianyi!
<!-- - *2024.11*: &nbsp;ğŸ‰ğŸ‰ Two papers were accepted to KDD 2025! Congratulations to Prof. Jingrui He's group at UIUC! -->
- *2024.10*: &nbsp;ğŸ‰ğŸ‰ Honored to be selected for funding by GDSTC General Program.
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ Two papers were accepted to NeurIPS 2024!
<!-- - *2024.06*: &nbsp;ğŸ‰ğŸ‰ Honored to receive the proposal grant RGC Early Career Scheme. -->
- *2023.09*: &nbsp;ğŸ‰ğŸ‰ One paper was accepted to NeurIPS 2023! Congratulations to Xiaoyuan and other collaborators from Prof. Qingfu Zhang's group at CityU!
- *2023.07*: I am looking for self-motivated **(visiting) Ph.D. students / (remote) research assistants** to work with me on machine learning. Fellowships / salaries are provided to qualified candidates. Any interested applicants can directly send me your **CV** and a brief introduction to your **research interest** to "ychen.stat.ML@outlook.com". Please refer to the [Zhihu post](https://zhuanlan.zhihu.com/p/651070296) for more details.
- *2023.04*: &nbsp;ğŸ‰ğŸ‰ I will join CS (and Math) @ Hong Kong Baptist University as an assistant professor in Fall 2023. 
- *2023.04*: &nbsp;ğŸ‰ğŸ‰ Two papers were accepted to ICML 2023!

# ğŸ“ Publications

\* Co-first author, âœ Mentee, âœ‰ï¸ Corresponding author

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening](https://arxiv.org/abs/2306.08854)

**Yifan Chen**, Rentian Yao, Yun Yang, Jie Chen

</div>
</div> -->

<!-- - ``KDD 2025`` [ResMoE: Space-efficient MoE Module Approximation via Wasserstein Barycenter and Residual Restoration](https://doi.org/10.1145/3690624.3709196)  
Mengting Ai\*âœ, Tianxin Wei\*, **Yifan Chen**\*âœ‰ï¸, Zhichen Zeng, Ritchie Zhao, Girish Varatkar, Bita Darvish Rouhani, Xianfeng Tang, Hanghang Tong, Jingrui Heâœ‰ï¸
- ``KDD 2025`` [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://doi.org/10.1145/3690624.3709280)  
Tianxin Wei\*, **Yifan Chen**\*, Xinrui He, Wenxuan Bao, Jingrui Heâœ‰ï¸ -->

- ``ICML 2025`` [A Recipe for Causal Graph Regression: Confounding Effects Revisited](NA)  
Yujia Yin\*âœ, Tianyi Qu\*âœ‰ï¸, Zihao Wang, **Yifan Chen**âœ‰ï¸
- ``ICML 2025`` [Towards Robust Influence Functions with Flat Validation Minima](NA)  
Xichen Yeâœ, Yifan Wuâœ, Weizhong Zhangâœ‰ï¸, Cheng Jinâœ‰ï¸, **Yifan Chen**âœ‰ï¸
- ``ISIT 2025`` [Embedding Empirical Distributions for Computing Optimal Transport Maps](https://arxiv.org/abs/2504.17740)  
Mingchen Jiang\*âœ, Peng Xu\*âœ‰ï¸, Xichen Ye, Xiaohui Chen, Yun Yang, **Yifan Chen**âœ‰ï¸
- ``ICASSP 2025`` [Catch Causal Signals from Edges for Label Imbalance in Graph Classification]([NA](https://doi.org/10.1109/ICASSP49660.2025.10889221))  
Fengrui Zhang\*âœ, Yujia Yin\*âœ, Hongzong Li, **Yifan Chen**âœ‰ï¸, Tianyi Quâœ‰ï¸
- ``AAAI 2025`` [Optimized Gradient Clipping for Noisy Label Learning](https://ojs.aaai.org/index.php/AAAI/article/view/33025)  
Xichen Yeâœ, Yifan Wuâœ, Weizhong Zhang, Xiaoqiang Liâœ‰ï¸, **Yifan Chen**âœ‰ï¸, Cheng Jin
- ``NeurIPS 2024`` [Gliding over the Pareto Front with Uniform Designs](https://neurips.cc/virtual/2024/poster/94814)  
Xiaoyuan Zhang, Genghui Li, Xi Lin, Yichi Zhang, **Yifan Chen**, Qingfu Zhang
- ``NeurIPS 2024`` [LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch](https://neurips.cc/virtual/2024/poster/97588)  
Xiaoyuan Zhang, Liang Zhao, Yingying Yu, Xi Lin, **Yifan Chen**, Han Zhao, Qingfu Zhang
- ``NeurIPS 2023`` [Hypervolume Maximization: A Geometric View of Pareto Set Learning](https://nips.cc/virtual/2023/poster/72581)  
Xiaoyuan Zhang, Bo Xue, Xi Lin, **Yifan Chen**âœ‰ï¸, Qingfu Zhangâœ‰ï¸
- ``ICML 2023`` [A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening](https://proceedings.mlr.press/v202/chen23ak.html)  
**Yifan Chen**, Rentian Yao, Yun Yang, Jie Chen
- ``ICML 2023`` [NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning](https://proceedings.mlr.press/v202/wei23b.html)  
Tianxin Wei\*, Zeming Guo\*âœ, **Yifan Chen**\*âœ‰ï¸, Jingrui Heâœ‰ï¸
- ``TMLR`` [Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks](https://openreview.net/forum?id=JyKNuoZGux)  
**Yifan Chen**\*, Tianning Xu\*, Dilek Hakkani-Tur, Di Jin, Yun Yang, Ruoqing Zhu. Transactions on Machine Learning Research (TMLR), 2023.
- ``EMNLP 2022`` <span style="color:red">(Oral)</span> [Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning](https://aclanthology.org/2022.emnlp-main.50/)  
**Yifan Chen**\*, Devamanyu Hazarika\*, Mahdi Namazifar, Yang Liu, Di Jin, Dilek Hakkani-Tur
- ``NAACL 2022`` <span style="color:red">(Oral)</span> [Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences](https://aclanthology.org/2022.naacl-main.381/)  
**Yifan Chen**\*, Qi Zeng\*, Dilek Hakkani-Tur, Di Jin, Heng Ji, Yun Yang
- ``NAACL 2022`` (Findings) [Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention](https://aclanthology.org/2022.findings-naacl.102/)  
**Yifan Chen**\*, Devamanyu Hazarika\*, Mahdi Namazifar, Yang Liu, Di Jin, Dilek Hakkani-Tur
- ``NeurIPS 2021`` [Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\"om Method](https://proceedings.neurips.cc/paper/2021/hash/10a7cdd970fe135cf4f7bb55c0e3b59f-Abstract.html)  
**Yifan Chen**\*, Qi Zeng\*, Heng Ji, Yun Yang
- ``AISTATS 2021`` [Accumulations of Projectionsâ€”A Unified Framework for Random Sketches in Kernel Ridge Regression](http://proceedings.mlr.press/v130/chen21f.html)  
**Yifan Chen**, Yun Yang
- ``AISTATS 2021`` [Fast Statistical Leverage Score Approximation in Kernel Ridge Regression](https://proceedings.mlr.press/v130/chen21e.html)  
**Yifan Chen**, Yun Yang

# ğŸ– Honors and Awards
- *2023.05* Dissertation Completion Fellowships (gratefully declined due to early graduation), USD **$25,000**, University of Illinois Graduate College
- *2023.05* The Fortieth International Conference on Machine Learning (ICML 2023) Grant Award, USD **$1,500**
- *2018.06* Shanghai Outstanding Graduate, Shanghai Municipal Education Commission

<!-- # ğŸ’¬ Professional Talks
- *2023.08*, A Gromov--Wasserstein Geometric View of Spectrum-Preserving Graph Coarsening, HKBU Math department invited talk, Hong Kong
- *2023.08*, NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning, the 1st International Conference on AI-generated Content (AIGC2023) invited talk, Remote
- *2022.08*, One Expert with Multiple Instruments, Microsoft Azure internal talk, Remote
- *2022.08*, Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences, AI Time PhD NAACL Special Session, Remote
- *2021.12*, Empowering parameter-efficient transfer learning by recognizing the kernel structure in self-attention, Amazon Alexa AI internal talk, Sunnyvale, CA, USA -->
